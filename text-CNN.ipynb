{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D, Flatten\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import text\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "import imdbb\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, count, hate_speech_count, offensive_language_count, neither_count, classs, tweet):\n",
    "        \"\"\"\n",
    "        - count (int): The total count of the tweet.\n",
    "        - hate (int): The count of hate speech in the tweet.\n",
    "        - offensive (int): The count of offensive language in the tweet.\n",
    "        - neither (int): The count of content classified as neither hate speech nor offensive.\n",
    "        - classs (str): The classification of the tweet.\n",
    "        - tweet (str): The text content of the tweet.\n",
    "        \"\"\"\n",
    "        self.count = count\n",
    "        self.hate = hate_speech_count\n",
    "        self.offensive = offensive_language_count\n",
    "        self.neither = neither_count\n",
    "        self.classs = classs\n",
    "        self.tweet = tweet\n",
    "        self.stems = []\n",
    "        self.tokens = []\n",
    "        self.code = []\n",
    "        self.tags = []\n",
    "        self.t_code = []\n",
    "        self.presence_word = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.count} ; {self.hate} ; {self.offensive} ; {self.neither} ; {self.classs} ;; {self.tweet}\"\n",
    "    \n",
    "    def peacefullness(self):\n",
    "        return self.neither_count / self.count\n",
    "    \n",
    "    def offensiveness(self):\n",
    "        return self.offensive / self.count\n",
    "    \n",
    "    def hateness(self):\n",
    "        return self.hate / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = []\n",
    "\n",
    "with open('./archive/train.csv', 'r+') as file:\n",
    "    previous_line = ''\n",
    "\n",
    "    # Initialize a list to accumulate the modified content\n",
    "    final_content_lines = []\n",
    "\n",
    "    # Read and accumulate non-empty lines\n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        if stripped_line and stripped_line[0].isdigit():\n",
    "            # If the current line is not empty and starts with an integer, accumulate it\n",
    "            final_content_lines.append(stripped_line)\n",
    "            previous_line = stripped_line\n",
    "        else:\n",
    "            # If the current line doesn't start with an integer, append it to the previous line\n",
    "            previous_line += stripped_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in final_content_lines:\n",
    "    \n",
    "    comma_indices = [index for index, char in enumerate(line) if char == ',']\n",
    "\n",
    "    # Extracting substrings between commas\n",
    "    count_str = line[0:comma_indices[0]].strip()\n",
    "    hate_str = line[comma_indices[0]+1:comma_indices[1]].strip()\n",
    "    offensive_str = line[comma_indices[1]+1:comma_indices[2]].strip()\n",
    "    neither_str = line[comma_indices[2]+1:comma_indices[3]].strip()\n",
    "    classs_str = line[comma_indices[3]+1:comma_indices[4]].strip()\n",
    "    tweet_str = line[comma_indices[4]+1:].strip()\n",
    "\n",
    "    # Converting to integers\n",
    "    count = int(count_str) if count_str.isdigit() else None\n",
    "    hate = int(hate_str) if hate_str.isdigit() else None\n",
    "    offensive = int(offensive_str) if offensive_str.isdigit() else None\n",
    "    neither = int(neither_str) if neither_str.isdigit() else None\n",
    "    classs = int(classs_str) if classs_str.isdigit() else None\n",
    "\n",
    "    # Creating an instance of the Tweet class\n",
    "    tweet_instance = Tweet(count, hate, offensive, neither, classs_str, tweet_str)\n",
    "\n",
    "    # Append the tweet instance to a list or do whatever you need to do with it\n",
    "    Tweets.append(tweet_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = imdbb.load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000 # number of words\n",
    "maxlen = 50 # maximum number of words in a sentence\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 100\n",
    "epochs = 10\n",
    "test_ratio = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "\n",
    "limit = int(len(Tweets) * ( 1 - test_ratio ))\n",
    "\n",
    "for t in range(limit):\n",
    "    X_train.append(Tweets[t].tweet)\n",
    "    if Tweets[t].hateness() > .5:\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        y_train.append(0)\n",
    "\n",
    "for t in range(limit, len(Tweets)):\n",
    "    X_test.append(Tweets[t].tweet)\n",
    "    if Tweets[t].hateness() > .5:\n",
    "        y_test.append(1)\n",
    "    else:\n",
    "        y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train)\n",
    "X_test = tokenizer.texts_to_matrix(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file_path = os.path.join('./tokenizer/', 'tokenizer.pickle')\n",
    "with open(tokenizer_file_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 1000.0, \tmin: 0, \tmax: 1000, \tlen(X_train): 19826\n",
      "y positive percentage balance: 6.007263189750832\n"
     ]
    }
   ],
   "source": [
    "avg = 0 \n",
    "max = 0\n",
    "min = 0\n",
    "\n",
    "for i in X_train:\n",
    "    l = len(i)\n",
    "    if l > max:\n",
    "        max = l\n",
    "    if l < min:\n",
    "        min = l\n",
    "\n",
    "    avg += l\n",
    "\n",
    "avg /= len(X_train)\n",
    "\n",
    "print(f\"avg: {avg}, \\tmin: {min}, \\tmax: {max}, \\tlen(X_train): {len(X_train)}\")\n",
    "\n",
    "count = 0\n",
    "for i in y_train:\n",
    "    if i == 1:\n",
    "        count+=1\n",
    "\n",
    "\n",
    "print(\"y positive percentage balance:\", 100* count / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train),\n",
    "                                        y = y_train                                                    \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 29s 45ms/step - loss: 0.6942 - accuracy: 0.5810 - val_loss: 0.6984 - val_accuracy: 0.0466\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 27s 44ms/step - loss: 0.6933 - accuracy: 0.1432 - val_loss: 0.6979 - val_accuracy: 0.0466\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 31s 50ms/step - loss: 0.6932 - accuracy: 0.0601 - val_loss: 0.6955 - val_accuracy: 0.0466\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 37s 59ms/step - loss: 0.6932 - accuracy: 0.7063 - val_loss: 0.6961 - val_accuracy: 0.0466\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 37s 60ms/step - loss: 0.6932 - accuracy: 0.0601 - val_loss: 0.6940 - val_accuracy: 0.0466\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 37s 60ms/step - loss: 0.6933 - accuracy: 0.3355 - val_loss: 0.6934 - val_accuracy: 0.0466\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 37s 60ms/step - loss: 0.6932 - accuracy: 0.8015 - val_loss: 0.6973 - val_accuracy: 0.0466\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 37s 60ms/step - loss: 0.6932 - accuracy: 0.1865 - val_loss: 0.6931 - val_accuracy: 0.9534\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 42s 68ms/step - loss: 0.6932 - accuracy: 0.7559 - val_loss: 0.6934 - val_accuracy: 0.0466\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 40s 65ms/step - loss: 0.6932 - accuracy: 0.7121 - val_loss: 0.6954 - val_accuracy: 0.0466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7efe8f934ca0>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1, \n",
    "          class_weight=class_weights,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('models/hate-v3-CNN-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'his' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_history(\u001b[43mhis\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'his' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(his)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
