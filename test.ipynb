{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './archive/train.csv'  # Update with the correct file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean the tweets\n",
    "data['tweet'] = data['tweet'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Preparing the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tweet'])\n",
    "\n",
    "# Convert the text to sequences\n",
    "X = tokenizer.texts_to_sequences(data['tweet'])\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "data['offensive_language_count_normalized'] = data['offensive_language_count'] / data['count']\n",
    "data['offensive_language_count_normalized'] = data['offensive_language_count_normalized'].apply(lambda x: 1 if x >= 0.9 else 0)\n",
    "\n",
    "# Prepare the target variable\n",
    "y = data['offensive_language_count_normalized'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "310/310 [==============================] - 67s 190ms/step - loss: 0.5475 - accuracy: 0.7396 - val_loss: 0.5338 - val_accuracy: 0.7716\n",
      "Epoch 2/5\n",
      "310/310 [==============================] - 56s 181ms/step - loss: 0.4048 - accuracy: 0.8330 - val_loss: 0.5357 - val_accuracy: 0.7571\n",
      "Epoch 3/5\n",
      "310/310 [==============================] - 56s 180ms/step - loss: 0.2872 - accuracy: 0.8847 - val_loss: 0.6675 - val_accuracy: 0.7295\n",
      "Epoch 4/5\n",
      "310/310 [==============================] - 56s 181ms/step - loss: 0.1855 - accuracy: 0.9276 - val_loss: 0.8091 - val_accuracy: 0.7125\n",
      "Epoch 5/5\n",
      "310/310 [==============================] - 56s 181ms/step - loss: 0.1249 - accuracy: 0.9529 - val_loss: 1.1282 - val_accuracy: 0.7095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f136ff06470>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Plus 1 for padding token\n",
    "embed_size = 128\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=50))\n",
    "model.add(LSTM(60, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(60))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 'sigmoid' for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=64, class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 32.776 %\n"
     ]
    }
   ],
   "source": [
    "def predict_hateness(message):\n",
    "    # Clean and preprocess the message\n",
    "    cleaned_message = clean_text(message)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_message])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=50)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(padded_sequence, verbose=0)\n",
    "    return prediction[0][0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    \"\",\n",
    "    \"miaou\",\n",
    "    \"hello\",\n",
    "    \"I'm not your friend\",\n",
    "    \"I'm your friend\",\n",
    "    \"Don't stop\",\n",
    "    \"You're a poop\",\n",
    "    \"You're a big big poop\",\n",
    "    \"just go\",\n",
    "    \"lmao\",\n",
    "    \"ig\",\n",
    "    \"tv\",\n",
    "    \"fuck you\",\n",
    "    \"love you\",\n",
    "    \"thank you\",\n",
    "    \"snorkel\",\n",
    "    \"phillipins\",\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "    \"just finish it\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\":\t32.78 %\n",
      "\"miaou\":\t32.78 %\n",
      "\"hello\":\t11.01 %\n",
      "\"I'm not your friend\":\t23.46 %\n",
      "\"I'm your friend\":\t10.13 %\n",
      "\"Don't stop\":\t81.62 %\n",
      "\"You're a poop\":\t1.39 %\n",
      "\"You're a big big poop\":\t8.40 %\n",
      "\"just go\":\t64.71 %\n",
      "\"lmao\":\t33.10 %\n",
      "\"ig\":\t12.20 %\n",
      "\"tv\":\t40.81 %\n",
      "\"fuck you\":\t45.10 %\n",
      "\"love you\":\t9.68 %\n",
      "\"snorkel\":\t32.78 %\n",
      "\"phillipins\":\t32.78 %\n",
      "\"dog\":\t23.77 %\n",
      "\"cat\":\t77.14 %\n",
      "\"just finish it\":\t33.13 %\n"
     ]
    }
   ],
   "source": [
    "for t in test:\n",
    "    print(f\"\\\"{t}\\\":\\t{predict_hateness(t):2.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
