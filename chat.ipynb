{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Goal is to make the computer understand the language<br>\n",
    "\n",
    "Here are the steps\n",
    "\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Stemming\n",
    "4. Dependency parsing\n",
    "5. Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare a proper storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, count, hate_speech_count, offensive_language_count, neither_count, classs, tweet):\n",
    "        \"\"\"\n",
    "        - count (int): The total count of the tweet.\n",
    "        - hate (int): The count of hate speech in the tweet.\n",
    "        - offensive (int): The count of offensive language in the tweet.\n",
    "        - neither (int): The count of content classified as neither hate speech nor offensive.\n",
    "        - classs (str): The classification of the tweet.\n",
    "        - tweet (str): The text content of the tweet.\n",
    "        \"\"\"\n",
    "        self.count = count\n",
    "        self.hate = hate_speech_count\n",
    "        self.offensive = offensive_language_count\n",
    "        self.neither = neither_count\n",
    "        self.classs = classs\n",
    "        self.tweet = tweet\n",
    "        self.stems = []\n",
    "        self.tokens = []\n",
    "        self.code = []\n",
    "        self.tags = []\n",
    "        self.t_code = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.count} ; {self.hate} ; {self.offensive} ; {self.neither} ; {self.classs} ;; {self.tweet}\"\n",
    "    \n",
    "    def peacefullness(self):\n",
    "        return self.neither_count / self.count\n",
    "    \n",
    "    def offensiveness(self):\n",
    "        return self.offensive / self.count\n",
    "    \n",
    "    def hateness(self):\n",
    "        return self.hate / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./archive/train.csv', 'r+') as file:\n",
    "    previous_line = ''\n",
    "\n",
    "    # Initialize a list to accumulate the modified content\n",
    "    final_content_lines = []\n",
    "\n",
    "    # Read and accumulate non-empty lines\n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        if stripped_line and stripped_line[0].isdigit():\n",
    "            # If the current line is not empty and starts with an integer, accumulate it\n",
    "            final_content_lines.append(stripped_line)\n",
    "            previous_line = stripped_line\n",
    "        else:\n",
    "            # If the current line doesn't start with an integer, append it to the previous line\n",
    "            previous_line += stripped_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in final_content_lines:\n",
    "    \n",
    "    comma_indices = [index for index, char in enumerate(line) if char == ',']\n",
    "\n",
    "    # Extracting substrings between commas\n",
    "    count_str = line[0:comma_indices[0]].strip()\n",
    "    hate_str = line[comma_indices[0]+1:comma_indices[1]].strip()\n",
    "    offensive_str = line[comma_indices[1]+1:comma_indices[2]].strip()\n",
    "    neither_str = line[comma_indices[2]+1:comma_indices[3]].strip()\n",
    "    classs_str = line[comma_indices[3]+1:comma_indices[4]].strip()\n",
    "    tweet_str = line[comma_indices[4]+1:].strip()\n",
    "\n",
    "    # Converting to integers\n",
    "    count = int(count_str) if count_str.isdigit() else None\n",
    "    hate = int(hate_str) if hate_str.isdigit() else None\n",
    "    offensive = int(offensive_str) if offensive_str.isdigit() else None\n",
    "    neither = int(neither_str) if neither_str.isdigit() else None\n",
    "    classs = int(classs_str) if classs_str.isdigit() else None\n",
    "\n",
    "    # Creating an instance of the Tweet class\n",
    "    tweet_instance = Tweet(count, hate, offensive, neither, classs_str, tweet_str)\n",
    "\n",
    "    # Append the tweet instance to a list or do whatever you need to do with it\n",
    "    Tweets.append(tweet_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ; 0 ; 0 ; 3 ; 2 ;; !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; !!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit\n",
      "3 ; 0 ; 2 ; 1 ; 1 ;; !!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny\n",
      "6 ; 0 ; 6 ; 0 ; 1 ;; !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;\n",
      "3 ; 1 ; 2 ; 0 ; 1 ;; \"!!!!!!!!!!!!!!!!!!\"\"@T_Madison_x: The shit just blows me..claim you so faithful and down for somebody but still fucking with hoes! &#128514;&#128514;&#128514;\"\"\"\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; \"!!!!!!\"\"@__BrighterDays: I can not just sit up and HATE on another bitch .. I got too much shit going on!\"\"\"\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; !!!!&#8220;@selfiequeenbri: cause I'm tired of you big bitches coming for us skinny girls!!&#8221;\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; \"\"\" &amp; you might not get ya bitch back &amp; thats that \"\"\"\n",
      "3 ; 1 ; 2 ; 0 ; 1 ;; \"\"\" @rhythmixx_ :hobbies include: fighting Mariam\"\"\n"
     ]
    }
   ],
   "source": [
    "for t in Tweets[:10]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet formatting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove all points when it's not the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_points(line):\n",
    "    l = list(line)\n",
    "\n",
    "    for i in range(len(l)):\n",
    "        if i < len(l) - 2 and l[i] == '.' and 'A' <= l[i+2] <= 'Z':\n",
    "            l[i] = ' '\n",
    "\n",
    "    line = ' '.join(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply first format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change:\n",
    "- all `@<username>` to `username`\n",
    "- all URLs to `weblink`\n",
    "- all `&amp` to `&`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    t.tweet = re.sub(r'@([a-zA-Z0-9_]+)', 'username', t.tweet).replace('username:', '') # replace first username\n",
    "    t.tweet = re.sub(r'http?://\\S+', 'weblink', t.tweet)\n",
    "    t.tweet = re.sub(r'&amp', '&', t.tweet)\n",
    "\n",
    "    if t.tweet and t.tweet[0] == '.':\n",
    "        t.tweet = t.tweet[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ; 0 ; 0 ; 3 ; 2 ;; As a woman you shouldnt complain about cleaning up your house. & as a man you should always take the trash out.\n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; boy dats cold.tyga dwn bad for cuffin dat hoe in the 1st place \n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; username Dawg You ever fuck a bitch & she start to cry. You be confused as shit\n",
      "3 ; 0 ; 2 ; 1 ; 1 ;; username she look like a tranny\n",
      "6 ; 0 ; 6 ; 0 ; 1 ;; The shit you hear about me might be true or it might be faker than the bitch who told it to ya 57361 \n",
      "3 ; 1 ; 2 ; 0 ; 1 ;;  The shit just blows me.claim you so faithful & down for somebody but still fucking with hoes 128514 128514 128514 \n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; I can not just sit up & HATE on another bitch. I got too much shit going on \n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; 8220 cause Im tired of you big bitches coming for us skinny girls 8221 \n",
      "3 ; 0 ; 3 ; 0 ; 1 ;; & you might not get ya bitch back & thats that \n",
      "3 ; 1 ; 2 ; 0 ; 1 ;; username :hobbies include: fighting Mariam\n"
     ]
    }
   ],
   "source": [
    "for t in Tweets:\n",
    "    remove_points(t.tweet)\n",
    "    t.tweet = t.tweet   .replace('RT', '').replace('!', ' ').replace('\"', '').replace(\"\\n\", ' ')\\\n",
    "                        .replace(';', ' ').replace('-', ' ').replace(' and ', ' & ').replace('\\'', '')\\\n",
    "                        .replace('?', '.').replace(',', '').replace('~', ' ').replace('|', ' ').replace('Â°', ' ')\\\n",
    "                        .replace('`', ' ').replace('~', ' ').replace('*', ' ').replace('+', ' ').replace('/', ' ')\\\n",
    "                        .replace(' # ', ' ').replace('http', ' ').replace('t.co', ' ').replace('\\\\', ' ').replace('&#', ' ')\n",
    "    for _ in range(4):\n",
    "        t.tweet = t.tweet.replace('  ', ' ').replace('..', '.').replace(' .', '.') # remove multiple points & space\n",
    "    \n",
    "    if t.tweet and t.tweet[0] == ' ':\n",
    "        t.tweet = t.tweet[1:]\n",
    "\n",
    "\n",
    "for t in Tweets[:10]:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change everything in **lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    t.tweet = t.tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change some **abbreviations**<br>\n",
    "Here it is important not to change all abbreviations. some abbreviations are used by 50% of population. our model will think they are different if we don't change those 50% - 50% to a 100% unique expressions. But on the other hand, some are used by all the population (eg. `lol`) so we can keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = {\n",
    "    'ninstagram'        : 'instagram',\n",
    "    'instagramgram'     : 'instagram',\n",
    "    'ig'                : 'instagram',\n",
    "    'strainstagramht'   : 'instagram',\n",
    "    'insta'             : 'instagram',\n",
    "    'rinstagramht'      : 'instagram',\n",
    "    'ninstagramguh'     : 'instagram',\n",
    "    'instagramz'        : 'instagram',\n",
    "    'sinstagramn'       : 'instagram',\n",
    "    'binstagramgest'    : 'instagram',\n",
    "    'pinstagram'        : 'instagram',\n",
    "    'linstagramht'      : 'instagram',\n",
    "    'ninstagramg'       : 'instagram',\n",
    "    'instagramh'        : 'instagram',\n",
    "    'instagramnor'      : 'instagram',\n",
    "    'ninstagramht'      : 'instagram',\n",
    "    'ninstagramgramga'  : 'instagram',\n",
    "    'finstagramht'      : 'instagram',\n",
    "    'binstagram'        : 'instagram',\n",
    "    'hinstagramh'       : 'instagram',\n",
    "    'ninstagramga'      : 'instagram',\n",
    "    'toninstagramht'    : 'instagram',\n",
    "    'minstagramht'      : 'instagram',\n",
    "    'minstagramt'       : 'instagram',\n",
    "    'dwn'               : 'down',\n",
    "    'dawn'              : 'down',\n",
    "    'ta'                : 'that',\n",
    "    'dat'               : 'that',\n",
    "    'dawg'              : 'dude',\n",
    "    'smh'               : 'head',\n",
    "    'fr'                : 'real',\n",
    "    'plz'               : 'please',\n",
    "    'tf'                : 'wtf',\n",
    "    'theyr'             : 'are',\n",
    "    'bc'                : 'because',\n",
    "    'af'                : 'lot',\n",
    "    'u'                 : 'you',\n",
    "    'ppl'               : 'people',\n",
    "    'dm'                : 'message',\n",
    "    'bf'                : 'friend',\n",
    "    'gt'                : 'getting',\n",
    "    'ya'                : 'yes',\n",
    "    'na'                : 'no',\n",
    "    'ur'                : 'your',\n",
    "    'tryna'             : 'to',\n",
    "    'lmfao'             : 'lmao',\n",
    "    'ive'               : 'have'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for old in abbr:\n",
    "        new = abbr[old]\n",
    "        t.tweet = t.tweet.replace( ' ' + old + ' ', ' ' + new + ' ' ) # add some space arround the world to avoid matching a part of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that reduce the **repetitions** (people use to add a lot of letters of type a lot of emojis)<br>This reduce the data to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repetition(s):\n",
    "    # Use regular expression to find repeated substrings\n",
    "    pattern = re.compile(r'(.+?)\\1{%d,}' % 2)\n",
    "    match = pattern.search(s)\n",
    "\n",
    "    # Reduce repetition to two occurrences\n",
    "    while match:\n",
    "        repeated_substring = match.group(1)\n",
    "        s = s.replace(match.group(), repeated_substring, 1)\n",
    "        match = pattern.search(s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    t.tweet = reduce_repetition(t.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    t.tokens += nltk.word_tokenize(t.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for i in range(len(t.tokens) - 1, 0, -1):\n",
    "        if len(t.tokens[i].strip()) == 0 or (len(t.tokens[i].strip()) == 1 and t.tokens[i].strip() != 'a' and t.tokens[i].strip() != 'i' and t.tokens[i].strip() != '&'):\n",
    "            t.tokens.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's **compare** original string aigains its stemmed version to see the difference and say if the **stemmatisation** is **effective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split according to multiple charachters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for tok in t.tokens:\n",
    "        t.stems.append(stemmer.stem(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for i in range(len(t.stems) - 1, 0, -1):\n",
    "        if len(t.stems[i].strip()) == 0:\n",
    "            t.stems.pop(i)\n",
    "        elif len(t.stems[i].strip()) == 1:\n",
    "            if 'a' < t.stems[i].strip() <= 'z'and t.stems[i].strip() != 'i':\n",
    "                t.stems.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for i in range(len(t.stems) - 1, 0, -1):\n",
    "        arr = t.stems[i].split('.')\n",
    "        t.stems[i] = arr[0]\n",
    "        for j in range(1, len(arr)):\n",
    "            t.stems.insert(i+j, arr[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-clean one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for i in range(len(t.stems)):\n",
    "        if t.stems[i] in abbr.keys():\n",
    "            t.stems[i] = abbr[t.stems[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so hoes that smoke are losers. yea. go on ig\n",
      "['so', 'hoes', 'that', 'smoke', 'are', 'losers', 'yea', 'go', 'on', 'ig']\n",
      "['so', 'hoe', 'that', 'smoke', 'are', 'loser', 'yea', 'go', 'on', 'instagram'] \n",
      "\n",
      "so: advmod -> hoe\n",
      "hoe: ROOT -> hoe\n",
      "that: mark -> are\n",
      "smoke: nsubj -> are\n",
      "are: ccomp -> hoe\n",
      "loser: amod -> yea\n",
      "yea: compound -> go\n",
      "go: attr -> are\n",
      "on: prep -> go\n",
      "instagram: pobj -> on\n"
     ]
    }
   ],
   "source": [
    "print(Tweets[12].tweet)\n",
    "print(Tweets[12].tokens)\n",
    "print(Tweets[12].stems, \"\\n\")\n",
    "\n",
    "doc = nlp(' '.join(Tweets[12].stems))\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.dep_} -> {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allow us to know the role of a word in the sentence<br>\n",
    "let's store those information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    doc = nlp(' '.join(t.stems))\n",
    "    for token in doc:\n",
    "        t.tags.append(token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESHOLD = 3\n",
    "code_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will record words if they are used more than `TRESHOLD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_w = {}\n",
    "\n",
    "for t in Tweets:\n",
    "    for s in t.stems:\n",
    "        if not s in used_w.keys():\n",
    "            used_w[s] =  1\n",
    "        else:\n",
    "            used_w[s] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sorted(used_w.items(), key=lambda item: item[1], reverse=True):\n",
    "    if v < TRESHOLD:\n",
    "        break\n",
    "    code_table.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5707\n"
     ]
    }
   ],
   "source": [
    "print(len(code_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode-Decode** functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc(word):\n",
    "    index = -1\n",
    "    try:\n",
    "        index = code_table.index(word)\n",
    "    except ValueError:\n",
    "        index = -1\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(index):\n",
    "    if index >= len(code_table) or index < 0: return ''\n",
    "    return code_table(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for s in t.stems:\n",
    "        t.code.append(enc(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **encode** also the **tags** for the words. It will be **easier** to **handle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tags = {}\n",
    "\n",
    "for t in Tweets:\n",
    "    for g in t.tags:\n",
    "        if not g in d_tags.keys():\n",
    "            d_tags[g]  = 1\n",
    "        else:\n",
    "            d_tags[g] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Here it's not necessary to **sort** the tags usage count but it's not slowing the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_code_table = []\n",
    "for k, v in sorted(d_tags.items(), key=lambda item: item[1], reverse=True):\n",
    "    tag_code_table.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nsubj', 'compound', 'ROOT', 'det', 'dobj', 'prep', 'pobj', 'amod', 'advmod', 'aux', 'ccomp', 'npadvmod', 'nummod', 'conj', 'poss', 'cc', 'attr', 'neg', 'advcl', 'appos', 'relcl', 'mark', 'xcomp', 'acomp', 'intj', 'prt', 'nmod', 'dep', 'punct', 'auxpass', 'dative', 'nsubjpass', 'predet', 'oprd', 'parataxis', 'pcomp', 'csubj', 'acl', 'expl', 'quantmod', 'preconj', 'agent', 'meta', 'case', 'csubjpass']\n"
     ]
    }
   ],
   "source": [
    "print(tag_code_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_enc(tag):\n",
    "    index = -1\n",
    "    try:\n",
    "        index = tag_code_table.index(tag)\n",
    "    except ValueError:\n",
    "        index = -1\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_dec(index):\n",
    "    if index >= len(tag_code_table) or index < 0: return ''\n",
    "    return tag_code_table(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweets:\n",
    "    for g in t.tags:\n",
    "        t.t_code.append(t_enc(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how our data looks so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+\n",
      "|   TAG    | tag CODE |   STEM   | stem CODE |\n",
      "+----------+----------+----------+-----------+\n",
      "|   prep   |    5     |    as    |     77    |\n",
      "|   det    |    3     |    a     |     1     |\n",
      "|   pobj   |    6     |  woman   |    369    |\n",
      "|  nsubj   |    0     |   you    |     4     |\n",
      "|   aux    |    9     | shouldnt |    990    |\n",
      "|   neg    |    17    | complain |    717    |\n",
      "|   ROOT   |    2     |  about   |     60    |\n",
      "|   prep   |    5     |  clean   |    645    |\n",
      "|  pcomp   |    35    |    up    |     35    |\n",
      "|   prt    |    25    |   your   |     20    |\n",
      "|   poss   |    14    |   hous   |    306    |\n",
      "|   dobj   |    4     |    &     |     7     |\n",
      "|    cc    |    15    |    as    |     77    |\n",
      "|   prep   |    5     |    a     |     1     |\n",
      "|   det    |    3     |   man    |     88    |\n",
      "|   pobj   |    6     |   you    |     4     |\n",
      "|  nsubj   |    0     |  should  |    213    |\n",
      "|   aux    |    9     |  alway   |    163    |\n",
      "|  advmod  |    8     |   take   |    122    |\n",
      "|  advcl   |    18    |   the    |     5     |\n",
      "|   det    |    3     |  trash   |     49    |\n",
      "|   dobj   |    4     |   out    |     56    |\n",
      "|    -     |    -     |    -     |     -     |\n",
      "| npadvmod |    11    |   boy    |    177    |\n",
      "|   det    |    3     |   that   |     8     |\n",
      "|   amod   |    7     |   cold   |    562    |\n",
      "|  appos   |    19    |   tyga   |    1556   |\n",
      "|  advmod  |    8     |   down   |    140    |\n",
      "|   ROOT   |    2     |   bad    |     89    |\n",
      "|   prep   |    5     |   for    |     24    |\n",
      "|   pobj   |    6     |  cuffin  |    2250   |\n",
      "|   det    |    3     |   that   |     8     |\n",
      "|  relcl   |    20    |   hoe    |     9     |\n",
      "|   prep   |    5     |    in    |     11    |\n",
      "|   det    |    3     |   the    |     5     |\n",
      "|   amod   |    7     |   1st    |    882    |\n",
      "|   pobj   |    6     |  place   |    501    |\n",
      "|    -     |    -     |    -     |     -     |\n",
      "|  nsubj   |    0     | usernam  |     2     |\n",
      "|   ROOT   |    2     |   dude   |    209    |\n",
      "|  nsubj   |    0     |   you    |     4     |\n",
      "|  advmod  |    8     |   ever   |    179    |\n",
      "|  ccomp   |    10    |   fuck   |     21    |\n",
      "|   det    |    3     |    a     |     1     |\n",
      "|   dobj   |    4     |  bitch   |     0     |\n",
      "|    cc    |    15    |    &     |     7     |\n",
      "|  nsubj   |    0     |   she    |     52    |\n",
      "|   conj   |    13    |  start   |    201    |\n",
      "|   aux    |    9     |    to    |     6     |\n",
      "|  xcomp   |    22    |   cri    |    340    |\n",
      "|  nsubj   |    0     |   you    |     4     |\n",
      "|  ccomp   |    10    |    be    |     17    |\n",
      "|  acomp   |    23    |  confus  |    900    |\n",
      "|   prep   |    5     |    as    |     77    |\n",
      "|   pobj   |    6     |   shit   |     46    |\n",
      "|    -     |    -     |    -     |     -     |\n",
      "| npadvmod |    11    | usernam  |     2     |\n",
      "|  nsubj   |    0     |   she    |     52    |\n",
      "|   ROOT   |    2     |   look   |     67    |\n",
      "|   prep   |    5     |   like   |     14    |\n",
      "|   det    |    3     |    a     |     1     |\n",
      "|   pobj   |    6     |  tranni  |    634    |\n",
      "|    -     |    -     |    -     |     -     |\n",
      "|   det    |    3     |   the    |     5     |\n",
      "|  nsubj   |    0     |   shit   |     46    |\n",
      "|  nsubj   |    0     |   you    |     4     |\n",
      "|  relcl   |    20    |   hear   |    461    |\n",
      "|   prep   |    5     |  about   |     60    |\n",
      "|   pobj   |    6     |    me    |     16    |\n",
      "|   aux    |    9     |  might   |    347    |\n",
      "|   ROOT   |    2     |    be    |     17    |\n",
      "|  acomp   |    23    |   true   |    530    |\n",
      "|    cc    |    15    |    or    |     76    |\n",
      "|  nsubj   |    0     |    it    |     12    |\n",
      "|   aux    |    9     |  might   |    347    |\n",
      "|   conj   |    13    |    be    |     17    |\n",
      "|  acomp   |    23    |  faker   |     -1    |\n",
      "|   prep   |    5     |   than   |    141    |\n",
      "|   det    |    3     |   the    |     5     |\n",
      "|   pobj   |    6     |  bitch   |     0     |\n",
      "|  nsubj   |    0     |   who    |     79    |\n",
      "|  relcl   |    20    |   told   |    286    |\n",
      "|   dobj   |    4     |    it    |     12    |\n",
      "|   prep   |    5     |    to    |     6     |\n",
      "|   pobj   |    6     |   yes    |     86    |\n",
      "|  nummod  |    12    |  57361   |     -1    |\n",
      "|    -     |    -     |    -     |     -     |\n",
      "+----------+----------+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable([\"TAG\", \"tag CODE\", \"STEM\", \"stem CODE\"])\n",
    "\n",
    "for t in Tweets[:5]:\n",
    "    for i in range(len(t.stems)):\n",
    "        table.add_row([t.tags[i], t.t_code[i], t.stems[i], t.code[i]])\n",
    "    table.add_row(['-', '-', '-', '-'])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for making a model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my ideas for the input of the model \n",
    "\n",
    "1. An array of this type:\n",
    "  - [$stem_0$, $tag_0$, $stem_1$, $tag_1$, $stem_2$, $tag_2$ ... $stem_n$, $tag_n$]\n",
    "\n",
    "2. Or an array of this type: \n",
    "  - [$stem_0$, $stem_1$, $stem_2$ ... $stem_n$, $tag_0$, $tag_1$, $tag_2$ ... $tag_n$]\n",
    "\n",
    "3. Or two input arrays:\n",
    "  - stem code\n",
    "  - tag code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** offers us the `Embedding` layer with those arguments:\n",
    "\n",
    "- input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "- output_dim: Integer. Dimension of the dense embedding.\n",
    "\n",
    "- embeddings_initializer: Initializer for the embeddings matrix (see keras.initializers).\n",
    "\n",
    "- embeddings_regularizer: Regularizer function applied to the embeddings matrix (see keras.regularizers).\n",
    "\n",
    "- embeddings_constraint: Constraint function applied to the embeddings matrix (see keras.constraints).\n",
    "\n",
    "- mask_zero: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out.\n",
    "\n",
    "This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "\n",
    "> Source: [Keras' documentation](https://keras.io/api/layers/core_layers/embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to figure the maximum integer index. in the 1st and 2nd case, it is equals to two times the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max   = 67 \n",
      "2*max = 134\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for t in Tweets:\n",
    "    if max < len(t.stems):\n",
    "        max = len(t.stems)\n",
    "\n",
    "print(\"max   =\", max, \"\\n2*max =\", 2*max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maximal input dimension of **500** seems enough and will allow the user to input a large text and **compute** it in the **model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
